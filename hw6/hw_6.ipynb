{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/gist/qbeer/07eb98879a555a676b6da86ea8cd7f9e/hw_6_raw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-Nr8CVzD4uC"
   },
   "source": [
    "## 1. Implement a linear model\n",
    "return the weight parameters w = (w1, w2, ... , wP) and the intercept parameter w0 separately where:\n",
    "\n",
    "![image.png](data:image/png;base64,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)\n",
    "\n",
    "* check your returned coefficients with the built in `LinearRegression` class from the sklearn library, they should be within tolerance 1e-6 to each other\n",
    "\n",
    "* use a generated regression dataset from `sklearn.dataset import make_regression` API with parameters `n_samples=1000` and `n_features=20`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression             #for generated dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler         #DO NOT EVER FORGET THIS!\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression        \n",
    "from sklearn.linear_model import Lasso, Ridge        \n",
    "from sklearn.linear_model import LogisticRegression      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0_X, data0_Y = make_regression(n_samples=1000, n_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data0_X.shape, data0_Y.shape) #seems fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Its in a numpy array format\n",
    "2. It has 20 features, thus I don't have to pop index columns, remove NaNs...\n",
    "3. I HAVE TO BUILD MINE....\n",
    "\n",
    "Guide links:\n",
    "\n",
    "https://in.springboard.com/blog/linear-regression-model/\n",
    "\n",
    "https://medium.com/@lope.ai/multivariate-linear-regression-from-scratch-in-python-5c4f219be6a\n",
    "\n",
    "So the second gives a really straightforward description with OLS and says that there could a gradient to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, Y, W):\n",
    "    \"\"\"\n",
    "    Calculates the cost\n",
    "        INPUT:\n",
    "        X: N x M shaped dataset\n",
    "        Y: N x 1 shaped dataset\n",
    "        W: starter coefficents\n",
    "    \"\"\"\n",
    "    m = len(Y)\n",
    "    J = np.sum((X.dot(W) - Y) ** 2)/(2 * m)\n",
    "    return J\n",
    "\n",
    "\n",
    "def batch_gradient_descent(X, Y, W, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Calculates multivariate linear regression with starter W_1,W_2,... initial guess\n",
    "    \n",
    "    Does StandardScaling of the data\n",
    "    \n",
    "    INPUT:\n",
    "        X: N x M shaped dataset\n",
    "        Y: N x 1 shaped dataset\n",
    "        W: starter coefficents\n",
    "        alpha: used for gradient\n",
    "        iterations: number of iterations\n",
    "    \n",
    "    \"\"\"\n",
    "    cost_history = [0] * iterations\n",
    "    m = len(Y)\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        sys.stdout.write('\\r' + str(iteration+1) + '/' + str(iterations))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Hypothesis Values\n",
    "        h = X.dot(W)\n",
    "\n",
    "        # Difference b/w Hypothesis and Actual Y\n",
    "        loss = h - Y\n",
    "\n",
    "        # Gradient Calculation\n",
    "        gradient = X.T.dot(loss) / m\n",
    "\n",
    "        # Changing Values of B using Gradient\n",
    "        W = W - alpha * gradient\n",
    "\n",
    "        # New Cost Value\n",
    "        cost = cost_function(X, Y, W)\n",
    "        cost_history[iteration] = cost\n",
    "\n",
    "    return W, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0_X_train, data0_X_test, data0_Y_train, data0_Y_test = train_test_split(data0_X, data0_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000"
     ]
    }
   ],
   "source": [
    "W = np.zeros(data0_X_train.shape[1])\n",
    "alpha = 0.005\n",
    "iter_ = 2000\n",
    "newW, cost_history = batch_gradient_descent(data0_X_train,  data0_Y_train, W, alpha, iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.23513060e+01,  5.83767844e+01, -4.72311352e-03,  1.58782690e-02,\n",
       "        4.79866153e+01, -6.88998705e-03,  5.85879327e-03,  2.31880033e-02,\n",
       "        8.71515602e+01,  1.61875214e+01,  9.81673604e+01,  9.47979696e+01,\n",
       "        7.27016087e+01,  4.48068120e-03, -7.61013563e-03,  1.16277937e-02,\n",
       "       -7.81716339e-03, -4.34302685e-03,  8.19003222e+01,  5.36533965e+01])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwvvQJFCD8Eq"
   },
   "source": [
    "## 2. Use of real data\n",
    "\n",
    "* download the [Communities and Crime Data Set](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime) from UCI, the task includes understanding the dataset: naming the appropiate data fields, handling missing values, etc.\n",
    "\n",
    "* split the data in training/test sets and fit a LinearRegression model with 5-fold cross-validation on top of it - compare training and testing scores (R^2 by default) for the different CV splits, print the mean score and its standard deviation\n",
    "\n",
    "* fit the best Lasso regression model with 5-fold grid search cross validation (GridSearchCV) on the parameters: alpha, normalize, max_iter and show the best parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Shrinkage\n",
    "\n",
    "* interpret Lasso models based on its descriptive parameters by the shrinkage method described during the lecture (make a plot and check the names of the features that are not eliminated by the penalty parameter) on the data we have here [ this is an explanatory data analysis problem, try to be creative ]\n",
    "\n",
    "* fit Ridge models and apply the shrinkage method as well, did you get what you expect?\n",
    "\n",
    "* do you think normalization is needed here? if so, do not forget to use it in the next tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Subset selection\n",
    "\n",
    "* Split the data to training and test sets and do recursice feature elimination until 10 remaining predictors with 5-fold cross-validated regressors (RidgeCV, LassoCV, ElasticNetCV) on the training set, plot their names and look up some of their meanings [ recursive feature elimination is part of sklearn but you can do it with a for loop if you whish ]\n",
    "\n",
    "* Do all models provide the same descriptors? Check their performance on the test set! Plot all model predictions compared to the y_test on 3 different plots, which model seems to be the best?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ElasticNet penalty surface\n",
    "* visualize the surface of the `objective(alpha, beta)`\n",
    "\n",
    " * parameters corresponding to the L1 and L2 regularizations. Select the best possible combination of the hyper-parameters that minimize the objective (clue: from scipy.optimize import minimize)\n",
    "\n",
    "* this task is similar to what you've seen during class, just not for MSE vs. single penalty parameter but MSE vs. two penalty parameters `alpha, beta`\n",
    "\n",
    "* interpret the overall results, do you think regularization is necessary at all? do you think linear models are powerful enough on this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNJfeC8q8fsR5o2JtAZDyNy",
   "include_colab_link": true,
   "name": "HW_6_raw.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
